"""Test suite for the DictModelStrategy in Quick LLM."""

from typing import Callable
from langchain_core.language_models import (
    FakeListChatModel,
    FakeListLLM,
    LanguageModelInput,
)
from langchain_core.runnables import RunnableLambda
import pytest
from pytest_mock import MockFixture

from quick_llm import ChainFactory
from quick_llm.strategies import DictModelStrategy
from ..test_models import TestOutput, TestOutputDictionary


class TestAdaptLLM:
    """Test suite for validating the adaptation of Large Language Models (LLMs)
    to support structured output using the DictModelStrategy and ensuring proper handling
    for both Pydantic models and dictionary-based models.
    """

    def __with_structured_output_support[T](
        self,
        mocker: MockFixture,
        model: type[T],
        response_lambda: Callable[[LanguageModelInput], T],
    ) -> list:
        """Adapt a mocked LLM with structured output support to produce the
        desired structured output.

        This helper method ensures that the DictModelStrategy properly adapts a
        mocked LLM that supports the `with_structured_output` method. It
        invokes the adapted strategy's runner multiple times and verifies that
        the structured output matches the provided model type.

        Args:
            mocker (MockFixture): A pytest-mock fixture used to create and
            manipulate mocked objects. model (type[T]): The target model type
            to which the LLM output will be adapted. response_lambda
            (Callable[[str], T]): A lambda function providing the structured
            output based on the input prompt.

        Returns:
            list: A list of outputs generated by the adapted LLM runner,
            matching the specified model type.
        """
        # Mock a llm that returns structured output
        structured_llm = RunnableLambda(response_lambda)
        mocked_llm = mocker.MagicMock(spec=FakeListLLM)
        # Configure the mock to return our structured_llm when
        # with_structured_output is called with include_raw=True
        mocked_llm.with_structured_output.return_value = structured_llm
        factory = (
            ChainFactory.for_structured_output(model)
            .use_language_model(mocked_llm)
            .use_structured_output(model)
        )

        strategy = DictModelStrategy(factory)
        runner = strategy.adapted_llm

        structured_result = structured_llm.invoke("Test prompt 1")
        assert structured_result is not None
        assert structured_result == response_lambda("Test prompt 1")

        # Check that with_structured_output was called
        mocked_llm.with_structured_output.assert_called_once_with(
            model, include_raw=True
        )
        # assert structured_llm.invoke.call_count == 2
        return [runner.invoke("Test prompt 1"), runner.invoke("Test prompt 2")]

    @pytest.mark.parametrize(
        "model_type",
        [TestOutput, TestOutputDictionary],
    )
    def test_supported_model[T](
        self,
        mocker: MockFixture,
        model_type: type[T],
    ):
        """Test that the DictModelStrategy correctly adapts an LLM with structured output support.

        This test verifies that when an LLM with the `with_structured_output` method is provided:
        - The method is called with the correct arguments.
        - The adapted runner produces the expected structured output.
        - The output matches the TestOutput model in type and content.
        """
        response = self.__with_structured_output_support(
            mocker,
            model_type,
            lambda prompt: {
                "raw": f'{{"answer": "Answer {str(prompt)[-1]}"}}',
                "parsed": None,
            },
        )
        # Verify instances and content
        assert isinstance(response[0], dict)
        assert response[0]["answer"] == "Answer 1"
        assert isinstance(response[1], dict)
        assert response[1]["answer"] == "Answer 2"

    def __without_structured_output_support[T](self, model: type[T]) -> list:
        """Adapt an LLM without structured output support to produce the desired structured output.

        This helper method verifies that the DictModelStrategy can adapt a
        Large Language Model (LLM) that doesn't natively support producing structured output,
        ensuring the output matches the expected model type and structure.

        Args:
            model (type[T]): The target model type to which the LLM output will be adapted.

        Returns:
            list: A list of outputs generated by the adapted LLM, matching the specified model type.
        """
        base_llm = FakeListChatModel(
            responses=['{"answer": "Answer 1"}', '{"answer": "Answer 2"}']
        )
        factory = (
            ChainFactory.for_structured_output(model)
            .use_language_model(base_llm)
            .use_structured_output(model)
        )
        strategy = DictModelStrategy(factory)  # pyright: ignore[reportArgumentType]
        runner = strategy.adapted_llm

        return [runner.invoke("Test prompt 1"), runner.invoke("Test prompt 2")]

    @pytest.mark.parametrize("model_type", [TestOutput, TestOutputDictionary])
    def test_not_supported_model[T](self, model_type: type[T]):
        """Test that the DictModelStrategy correctly adapts an LLM with structured output support.

        This test verifies that when an LLM with the `with_structured_output` method is provided:
        - The method is called with the correct arguments.
        - The adapted runner produces the expected structured output.
        - The output matches the TestOutput model in type and content.
        """
        response = self.__without_structured_output_support(model_type)
        # Verify instances and content
        assert isinstance(response[0], dict)
        assert response[0]["answer"] == "Answer 1"
        assert isinstance(response[1], dict)
        assert response[1]["answer"] == "Answer 2"
